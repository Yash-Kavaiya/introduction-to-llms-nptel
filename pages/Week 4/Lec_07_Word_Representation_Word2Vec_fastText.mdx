# Lec 07 : Word Representation: Word2Vec & fastText

# Word Representation in Language Models ðŸ”¤ðŸ§ 

## 1. The Meaning of Words ðŸ“š

### Definition: *meaning* (Webster dictionary) ðŸ“–
- ðŸ”¹ The idea that is represented by a word, phrase, etc.
- ðŸ”¹ The idea that a person wants to express by using words, signs, etc.
- ðŸ”¹ The idea that is expressed in a work of writing, art, etc.

## 2. Need for Word Representation ðŸ”

For effective language modeling:
- ðŸ”¹ We need **effective representation** of words
- ðŸ”¹ The representation must somehow **encapsulate the word meaning**

## 3. Traditional Approach: Words as Discrete Symbols ðŸ·ï¸

In traditional NLP, words are treated as discrete symbols:
- Example: hotel, conference, motel â€“ a *localist representation*

Such symbols can be represented by **one-hot vectors**:
```
motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
```

> ðŸ“ **Note:** Vector dimension = number of words in vocabulary (e.g., 500,000+)
> One position contains 1, all others contain 0s

## 4. Problems with Discrete Symbol Representation âš ï¸

### Example Scenario:
In web search, if a user searches for "Delhi motel", we would also like to match documents containing "Delhi hotel"

### The Problem:
```
motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
```

| Issue | Explanation |
|-------|-------------|
| Orthogonality | These two vectors are completely orthogonal |
| No similarity | There is no natural notion of similarity for one-hot vectors |
| Potential solution | Could try to rely on WordNet's list of synonyms to get similarity |

## 5. Using Existing Thesauri: WordNet ðŸ“˜

### WordNet 3.0 Statistics

| Category | Unique Strings |
|----------|---------------|
| Noun | 117,798 |
| Verb | 11,529 |
| Adjective | 22,479 |
| Adverb | 4,481 |

### Definition of "Sense" in WordNet ðŸ”

- Uses **synset** (synonym set) - a set of near-synonyms that instantiates a sense or concept
- Includes a **gloss** (definition)

#### Example:
- "chump" as a noun with the gloss: "a person who is gullible and easy to take advantage of"
- This sense of "chump" is shared by 9 words: chumpÂ¹, foolÂ², gullÂ¹, markâ¹, patsyÂ¹, fall guyÂ¹, suckerÂ¹, soft touchÂ¹, mugÂ²
- Each of these words (in these specific senses) has the same gloss
- Note: Not every sense is shared; e.g., sense 2 of "gull" refers to the aquatic bird

### Multiple Senses Example: 'bass'
WordNet captures different meanings of the same word

## 6. Drawbacks of Thesaurus-based Approaches ðŸš«

| Limitation | Example |
|------------|---------|
| Missing nuance | "proficient" is listed as a synonym for "good" (only correct in some contexts) |
| Offensive terms | WordNet lists offensive synonyms without coverage of connotations or appropriateness |
| Missing new meanings | Missing modern usages: "wicked," "badass," "nifty," "wizard," "genius," "ninja," "bombest" |
| Maintenance challenges | Impossible to keep up-to-date with evolving language |
| Subjectivity | Requires human labor to create and adapt |

## Word Representation Evolution ðŸš€

```mermaid
graph LR
    A[Discrete Symbols] -->|Problems| B[Thesaurus-Based]
    B -->|Limitations| C[Distributional Semantics]
    C -->|Modern Approach| D[Word Embeddings]
    D --> E[Contextual Embeddings]
```

> ðŸ’¡ **Key Insight:** The evolution of word representation techniques shows a progression from simple symbolic approaches to more nuanced semantic models that better capture meaning and context.
>
> # ðŸ“Š Representing Words by Their Context ðŸ”

## Distributional Semantics: The Company Words Keep ðŸ” ðŸ‘¥

> ### ðŸ’¬ "You shall know a word by the company it keeps" 
> *â€” J. R. Firth (1957: 11)*

## How Context Defines Meaning ðŸ§©

**Distributional semantics** operates on a fundamental principle: **a word's meaning is given by the words that frequently appear close-by**.

### What is Context? ðŸ”Ž

- ðŸ”¹ When a word **w** appears in a text, its **context** is the set of words that appear nearby (within a fixed-size window)
- ðŸ”¹ Multiple contexts of **w** help build a comprehensive representation
- ðŸ”¹ Context windows capture the "neighboring words" that give meaning

## Example: Contexts for "banking" ðŸ¦

| Context Example | ... | Context Before | **Target Word** | Context After | ... |
|-----------------|-----|----------------|-----------------|---------------|-----|
| Example 1 | ... | government debt problems turning into | **banking** | crises as happened in 2009 | ... |
| Example 2 | ... | saying that Europe needs unified | **banking** | regulation to replace the hodgepodge | ... |
| Example 3 | ... | India has just given its | **banking** | system a shot in the arm | ... |

> ðŸ’¡ **Key Insight:** These context words collectively will represent the meaning of "banking"

## Visualizing the Context Window Concept ðŸ–¼ï¸

```mermaid
graph LR
    A["Context Before"] --- B["Target Word"]
    B --- C["Context After"]
    
    subgraph "Context Window"
    A
    B
    C
    end
```

## How Distributional Semantics Works ðŸ”„

| Traditional Approach | Distributional Approach |
|----------------------|-------------------------|
| ðŸ“š Dictionary definitions | ðŸ“Š Statistical patterns in text |
| ðŸ”¤ Fixed word meanings | ðŸ”„ Dynamic contextual meanings |
| ðŸ‘¨â€ðŸ’¼ Human-created | ðŸ¤– Data-driven |
| â±ï¸ Static, slow to update | ðŸ”„ Evolves with language use |

> ðŸ“˜ **Note:** This approach forms the foundation of modern word embeddings and contextual representation models that power today's language AI systems.

## Advantages of Context-Based Representation ðŸŒŸ

- ðŸ”¹ **Captures nuance**: Different contexts reveal different aspects of meaning
- ðŸ”¹ **Automatic**: Derived from data rather than manual construction
- ðŸ”¹ **Comprehensive**: Accommodates both common and rare usages
- ðŸ”¹ **Adaptable**: Can evolve as language changes

- # ðŸ“Š Count-based Methods for Word Representation ðŸ”¢

## Term-Context Matrix (Word-Word Matrix) ðŸ“ˆ

> The co-occurrence patterns between words can reveal their semantic relationships

### Matrix Structure ðŸ”

- ðŸ”¹ **Rows**: Target words
- ðŸ”¹ **Columns**: Context words
- ðŸ”¹ **Cells**: Number of times row word and column word co-occur in some context

### Context Types ðŸ“

Context can be defined as:
- ðŸ“„ Paragraph
- ðŸ” Window of 10 words
- ðŸ“Š Sentence or document

### Vector Representation ðŸ“Š

Each word becomes a **count vector** in $N_v$ (where V = vocabulary size):

| | aardvark | computer | data | pinch | result | sugar | ... |
|-|----------|----------|------|-------|--------|-------|-----|
| **apricot** | 0 | 0 | 0 | 1 | 0 | 1 | |
| **pineapple** | 0 | 0 | 0 | 1 | 0 | 1 | |
| **digital** | 0 | 2 | 1 | 0 | 1 | 0 | |
| **information** | 0 | 1 | 6 | 0 | 4 | 0 | |

## Sample Contexts from Brown Corpus ðŸ“š

> Examples of 20-word windows showing word co-occurrences

```
â€¢ equal amount of sugar, a sliced lemon, a tablespoonful of apricot preserve or jam, a 
  pinch each of clove and nutmeg,

â€¢ on board for their enjoyment. Cautiously she sampled her first pineapple and another 
  fruit whose taste she likened to that of

â€¢ of a recursive type well suited to programming on the digital computer. In finding the 
  optimal R-stage policy from that of

â€¢ substantially affect commerce, for the purpose of gathering data and information 
  necessary for the study authorized in the first section of this
```

## Word Similarity Through Context Vectors ðŸ¤

> **Key Principle**: Two words are similar in meaning if their context vectors are similar

### Example of Similar Words:
- ðŸ‘ "apricot" and ðŸ "pineapple" have identical context vectors
- ðŸ’» "digital" and ðŸ“Š "information" have different patterns

```mermaid
graph TD
    A["Context Vectors"] --> B["Similar Patterns"]
    A --> C["Different Patterns"]
    B --> D["Semantically Similar Words"]
    C --> E["Semantically Different Words"]
    D --> F["apricot/pineapple"]
    E --> G["digital/information"]
```

## Limitations of Raw Counts âš ï¸

### Problems with Raw Frequency:
- ðŸ“‰ **Skewed distribution**: Common words dominate
- ðŸ”¤ Words like "the" and "of" are very frequent but not discriminative
- ðŸŽ¯ Need measures that identify particularly informative context words

## Improved Measurement: TF-IDF Approach ðŸ“

### Term Frequency (TF) ðŸ“ˆ
- **Raw count**: $tf_{t,d} = count(t,d)$
- **Log-adjusted**: $tf_{t,d} = log_{10}(count(t,d)+1)$

### Document Frequency (DF) ðŸ“Š
- $df_t$ = number of documents term $t$ occurs in
- Not to be confused with collection frequency (total count across all documents)
- Example: "Romeo" is very distinctive for one Shakespeare play

### Inverse Document Frequency (IDF) ðŸ”„
- Measures how informative a word is
- Gives higher weight to rare terms that appear in few documents

| Measure | Purpose | Formula |
|---------|---------|---------|
| TF | Capture word importance in document | $tf_{t,d} = log_{10}(count(t,d)+1)$ |
| IDF | Downweight common words | $idf_t = log_{10}(\frac{N}{df_t})$ |
| TF-IDF | Combined importance measure | $tf\text{-}idf_{t,d} = tf_{t,d} \times idf_t$ |

> ðŸ’¡ **Key Insight**: TF-IDF balances the frequency of a term with its uniqueness across documents, providing a more informative measure than raw counts.
>
> # ðŸ§  Skip-gram Model in Word2Vec ðŸ“š

## Mathematical Foundation of Skip-gram ðŸ“Š

### Probability Equations

$$P(+|w, c) = \sigma(c \cdot w) = \frac{1}{1 + \exp(-c \cdot w)}$$

$$P(-|w, c) = 1 - P(+|w, c) = \sigma(-c \cdot w) = \frac{1}{1 + \exp(c \cdot w)}$$

> ðŸ” **Key Insight**: These equations calculate the probability of a word appearing in (or not appearing in) the context of another word

## How Skip-gram Classifier Computes Probabilities ðŸ§®

### For a Single Context Word
$$P(+|w, c) = \sigma(c \cdot w) = \frac{1}{1 + \exp(-c \cdot w)}$$

### For Multiple Context Words
- ðŸ”¹ When we have multiple context words, we need to consider all of them
- ðŸ”¹ We'll assume independence and multiply the probabilities:

$$P(+|w, c_{1:L}) = \prod_{i=1}^{L} \sigma(c_i \cdot w)$$

- ðŸ”¹ Taking the logarithm (for computational stability):

$$\log P(+|w, c_{1:L}) = \sum_{i=1}^{L} \log \sigma(c_i \cdot w)$$

## Skip-gram Training Process ðŸ”„

### Training Data Example
For a +/- 2 word window in the sentence:
```
... lemon, a [ tablespoon of apricot jam, a ] pinch ...
```

```mermaid
graph LR
    A["lemon,"] --- B["a"]
    B --- C["tablespoon"]
    C --- D["of"]
    D --- E["apricot"]
    E --- F["jam,"]
    F --- G["a"]
    G --- H["pinch"]
    
    subgraph "Context Window"
    C
    D
    E["apricot (target)"]
    F
    G
    end
```

### Skip-gram Classifier Goal ðŸŽ¯

Given a candidate (word, context) pair such as:
| Positive Example | Negative Example |
|------------------|------------------|
| (apricot, jam) | (apricot, aardvark) |

The classifier assigns each pair a probability:
- $P(+ | w, c)$ - probability that c is a valid context word for w
- $P(- | w, c) = 1 - P(+ | w, c)$ - probability that c is not a valid context word for w

## Computing Similarity with Dot Products âœ–ï¸

| Method | Formula | Properties |
|--------|---------|------------|
| Dot Product | $w \cdot c$ | Basic similarity measure |
| Cosine Similarity | $\frac{w \cdot c}{||w|| \cdot ||c||}$ | Normalized dot product |

> ðŸ’¡ **Key Insight**: Two vectors are similar if they have a high dot product

### Converting to Probabilities ðŸ”„

- ðŸ”¹ Similarity(w, c) âˆ wÂ·c 
- ðŸ”¹ To convert this similarity into a probability, we use the sigmoid function:

$$\sigma(x) = \frac{1}{1 + \exp(-x)}$$

```mermaid
graph TD
    A["Dot Product (wÂ·c)"] --> B["Sigmoid Function Ïƒ(wÂ·c)"]
    B --> C["Probability P(+|w,c)"]
```

## Skip-gram Classifier: Complete Picture ðŸ–¼ï¸

### Components
- ðŸŽ¯ **Input**: Target word w and its context window of L words $c_{1:L}$
- ðŸ“Š **Process**: Estimate probability based on vector similarity
- ðŸ”¢ **Output**: Probability of w occurring with these context words

### The Two Sets of Embeddings Needed ðŸ§©

| Embedding Type | Description | Example |
|----------------|-------------|---------|
| Target Word Embeddings | Vectors for center words | $w_{apricot}$ |
| Context Word Embeddings | Vectors for surrounding words | $c_{jam}$, $c_{tablespoon}$ |

> ðŸ“ **Note**: In Word2Vec, each word has two separate vectors - one for when it acts as a target word and one for when it acts as a context word

## Visual Summary of Skip-gram Architecture ðŸ—ï¸

```mermaid
graph TD
    A["Input Layer (One-hot vector)"] --> B["Embedding Layer (Target Embeddings)"]
    B --> C["Output Layer (Context Embeddings)"]
    C --> D["Sigmoid Function"]
    D --> E["Probability P(+|w,c)"]
    
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
```

> ðŸ”‘ **Key Advantage**: This approach generates dense vector representations that capture semantic relationships between words based on their distributional properties
>
> 
