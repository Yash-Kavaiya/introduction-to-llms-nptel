# Assignment 12

# Bias in Large Language Models (LLMs)

## Question
Which statements correctly characterize "bias" in the context of LLMs?
1. Bias can generate objectionable or stereotypical views in model outputs.
2. Bias is always intentionally introduced by malicious data curators.
3. Bias can cause harmful real-world impacts such as reinforcing discrimination.
4. Bias only affects low-resource languages; high-resource languages are unaffected.

## Options
a. 1 and 2
b. 1 and 3
c. 2 and 4
d. 1, 3, and 4

## Correct Answer: b (1 and 3)

## Detailed Explanation

Let's examine each statement systematically:

### Statement 1: Bias can generate objectionable or stereotypical views in model outputs. ✓ CORRECT
- LLMs learn patterns from their training data, which often contains existing societal biases
- These biases can manifest as stereotypical, prejudiced, or objectionable content in generated text
- Example: A model might associate certain professions predominantly with specific genders if such associations are overrepresented in its training data
- This is a well-documented challenge in NLP and AI ethics research

### Statement 2: Bias is always intentionally introduced by malicious data curators. ✗ INCORRECT
- Bias is typically **unintentional** and emerges from various sources:
  - Pre-existing societal biases reflected in text corpora
  - Historical imbalances in representation
  - Sampling methods that overrepresent certain perspectives
  - Structural inequalities in knowledge production
- While malicious introduction is possible, most bias issues stem from systemic factors rather than deliberate manipulation

### Statement 3: Bias can cause harmful real-world impacts such as reinforcing discrimination. ✓ CORRECT
- When deployed in real-world applications, biased LLMs can:
  - Perpetuate existing stereotypes and prejudices
  - Amplify discrimination in decision-making processes
  - Create representational harms for marginalized groups
  - Normalize unfair treatment across various domains (hiring, lending, content moderation, etc.)
- These impacts extend beyond the model itself to affect individuals and communities

### Statement 4: Bias only affects low-resource languages; high-resource languages are unaffected. ✗ INCORRECT
- Bias exists across all languages, including high-resource ones like English
- While low-resource languages may face additional challenges due to limited training data and representation, high-resource languages still exhibit significant biases
- The nature and manifestation of bias may differ between language contexts, but no language is inherently immune
- In fact, extensive research on bias has been conducted in high-resource languages, demonstrating clear evidence of various biases

Therefore, options 1 and 3 correctly characterize bias in LLMs, making answer choice **b** correct.

# Stereotype Score (ss) in Language Models

## Question
The Stereotype Score (ss) refers to:
a. The frequency with which a language model rejects biased associations.
b. The measure of how often a model's predictions are meaningless as opposed to meaningful.
c. A ratio of positive sentiment to negative sentiment in model outputs.
d. The proportion of examples in which a model chooses a stereotypical association over an anti-stereotypical one.

## Correct Answer: d

## Detailed Explanation

Let's systematically analyze each option to understand why option d is correct and why the others are incorrect:

### Option a: The frequency with which a language model rejects biased associations. ❌
This is incorrect because the Stereotype Score measures the opposite - it quantifies how often a model **accepts** or **produces** stereotypical associations, not how often it rejects them. A high Stereotype Score indicates more stereotypical outputs, not more rejections of bias.

### Option b: The measure of how often a model's predictions are meaningless as opposed to meaningful. ❌
This is incorrect because the Stereotype Score doesn't evaluate the meaningfulness of model outputs. It specifically focuses on measuring bias through stereotypical vs. non-stereotypical associations, regardless of whether those outputs are semantically meaningful.

### Option c: A ratio of positive sentiment to negative sentiment in model outputs. ❌
This is incorrect because the Stereotype Score doesn't measure sentiment polarity. While sentiment analysis is a different type of evaluation in NLP, the Stereotype Score specifically measures the model's tendency to produce socially stereotypical associations rather than evaluating emotional tone.

### Option d: The proportion of examples in which a model chooses a stereotypical association over an anti-stereotypical one. ✓
This is correct. The Stereotype Score is calculated by presenting a model with paired continuations or associations - one that aligns with social stereotypes and one that challenges or contradicts them - and measuring how frequently the model prefers the stereotypical option.

## Technical Implementation
In practice, the Stereotype Score is often calculated through experiments where:
1. Researchers create paired prompt completions (one stereotypical, one anti-stereotypical)
2. Both completions are presented to the model as potential continuations
3. The model's probability assignment or preference is recorded
4. The final score represents the fraction or percentage of cases where the model favored the stereotypical association

A Stereotype Score of 0.5 would indicate no bias (random choice between stereotypical and anti-stereotypical options), while scores significantly above 0.5 indicate bias toward stereotypes, and scores below 0.5 indicate bias against stereotypes (preference for anti-stereotypical associations).

This metric is valuable for quantifying and tracking bias in language models across different demographic attributes, domains, and model versions.

# Sources of Bias in Large Language Models (LLMs)

## Question
Which of the following are prominent sources of bias in LLMs?
1. Improper selection of training data leading to skewed distributions.
2. Reliance on older datasets causing "temporal bias."
3. Overemphasis on low-resource languages causing "linguistic inversion."
4. Unequal focus on high-resource languages resulting in "cultural bias."

## Answer Options
a. 1 and 2 only
b. 2 and 3 only
c. 1, 2, and 4
d. 1, 3, and 4

## Correct Answer: c (1, 2, and 4)

## Detailed Explanation

Let's examine each potential source of bias systematically:

### Statement 1: Improper selection of training data leading to skewed distributions. ✓ CORRECT
- This is a fundamental source of bias in LLMs
- When training data doesn't proportionally represent diverse demographics, viewpoints, or topics, the resulting model inherits these imbalances
- Examples include:
  - Overrepresentation of certain demographic groups
  - Imbalanced political or ideological content
  - Disproportionate representation of specific domains or topics
- These skewed distributions directly impact what patterns the model learns and subsequently generates
- Research has consistently shown that data selection has profound effects on model fairness

### Statement 2: Reliance on older datasets causing "temporal bias." ✓ CORRECT
- Historical text data often contains outdated social norms, beliefs, and factual information
- When models learn from this data, they inherit these temporal biases
- Examples include:
  - Outdated gender roles and stereotypes
  - Historical prejudices that are no longer socially acceptable
  - Obsolete factual information or scientific understanding
  - Language usage patterns that have evolved over time
- Models without regular updates can perpetuate these outdated perspectives
- This is why model knowledge cutoff dates are important contextual information

### Statement 3: Overemphasis on low-resource languages causing "linguistic inversion." ✗ INCORRECT
- This statement mischaracterizes the actual problem in LLM development
- The predominant issue is precisely the opposite: underrepresentation of low-resource languages
- "Linguistic inversion" is not a standard or recognized term in LLM bias literature
- Most LLMs suffer from insufficient training data for less commonly spoken languages
- This leads to poorer performance, not overemphasis, on low-resource languages
- The actual bias problem is the lack of linguistic diversity, not excessive focus on minority languages

### Statement 4: Unequal focus on high-resource languages resulting in "cultural bias." ✓ CORRECT
- LLMs are typically trained on datasets dominated by high-resource languages (particularly English)
- This creates several problems:
  - Poorer performance on low-resource languages
  - Western-centric perspectives and cultural assumptions
  - Misrepresentation or erasure of non-Western cultural contexts
  - Inability to understand culturally-specific references, idioms, or concepts
- These biases can marginalize users from non-dominant language communities
- Cultural bias extends beyond language to include cultural norms, values, and knowledge systems

Therefore, statements 1, 2, and 4 correctly identify prominent sources of bias in LLMs, making option c the correct answer.

# Adversarial Triggers for Bias Mitigation in LLMs

## Question
In the context of bias mitigation based on adversarial triggers, which best describes the goal of prepending specially chosen tokens to prompts?

## Options
a. To directly fine-tune the model parameters to remove bias
b. To override all prior knowledge in a model, effectively "resetting" it
c. To exploit the model's distributional patterns, thereby neutralizing or flipping biased associations in generated text
d. To randomly shuffle the tokens so that the model becomes more robust

## Correct Answer: c

## Detailed Explanation

Let's analyze each option to understand why option c is correct and why the others are incorrect:

### Option a: To directly fine-tune the model parameters to remove bias ❌
This option is incorrect because adversarial triggers do not modify the model's parameters at all. Fine-tuning involves updating model weights through additional training with gradient descent, which is not what happens when prepending tokens to prompts. Adversarial triggers work at inference time without changing the underlying model architecture or weights.

### Option b: To override all prior knowledge in a model, effectively "resetting" it ❌
This option is incorrect because adversarial triggers don't erase or reset the model's knowledge. They don't aim to make the model forget what it has learned but rather to redirect how it utilizes that knowledge. The model's parameters and learned representations remain intact when using adversarial triggers.

### Option c: To exploit the model's distributional patterns, thereby neutralizing or flipping biased associations in generated text ✓
This option is correct. Adversarial triggers work by leveraging the model's existing learned distributions and attention patterns. These carefully crafted token sequences:
- Create specific activation patterns within the model
- Shift probability distributions away from biased associations
- Exploit the model's own internal representations to counteract unwanted tendencies
- Work within the existing parameter space rather than changing it
- Function like "computational antidotes" to bias without retraining

### Option d: To randomly shuffle the tokens so that the model becomes more robust ❌
This option is incorrect because adversarial triggers are not random but precisely engineered. They are specifically designed through optimization processes to target particular biases. Random shuffling would likely degrade model performance rather than mitigating bias in a controlled way.

## Technical Context
Adversarial triggers for bias mitigation typically work through techniques like:
1. Optimization algorithms that search for token sequences that minimize biased outputs
2. Gradient-based approaches that identify tokens with maximum influence on reducing bias
3. Contrastive methods that push outputs away from stereotypical associations
4. Context engineering that creates distributional shifts in the model's internal representations

Unlike interventions that require model retraining or architecture changes, adversarial triggers provide a lightweight, inference-time solution that can be applied to frozen models.

# The "Regard" Metric in Language Models

## Question
Which of the following best describes the "regard" metric?
a. It is a measure of how well a model can explain its internal decision process.
b. It is a measurement of a model's perplexity on demographically sensitive text.
c. It is the proportion of times a model self-corrects discriminatory language.
d. It is a classification label reflecting the attitude towards a demographic group in the generated text.

## Correct Answer: d

## Detailed Explanation

### Option d: It is a classification label reflecting the attitude towards a demographic group in the generated text. ✓ CORRECT
- The regard metric evaluates the sentiment or attitude expressed toward specific demographic groups in model-generated text
- It typically classifies text into categories such as "positive," "negative," or "neutral" regard
- This classification helps identify whether certain demographics systematically receive more negative or disrespectful language than others
- Researchers use this metric to quantify bias by comparing regard scores across different demographic groups (e.g., comparing regard toward different genders, races, or religions)
- For example, if the model consistently generates text with negative regard for one demographic but positive regard for another when given similar prompts, this indicates bias

### Option a: It is a measure of how well a model can explain its internal decision process. ❌ INCORRECT
- This description refers to explainability or interpretability metrics, not regard
- Explainability focuses on making a model's decision-making process transparent and understandable
- While important for AI ethics, this is a separate concept from measuring demographic attitudes in text
- Regard does not evaluate a model's ability to explain itself but rather evaluates the content it produces

### Option b: It is a measurement of a model's perplexity on demographically sensitive text. ❌ INCORRECT
- Perplexity is a measure of how well a language model predicts a sequence of text (essentially how "surprised" the model is by the text)
- While perplexity can be measured on demographic text, this is not what regard measures
- Perplexity evaluates prediction accuracy, not attitudes or sentiment toward specific groups
- Regard focuses specifically on the qualitative nature of generated content rather than statistical prediction metrics

### Option c: It is the proportion of times a model self-corrects discriminatory language. ❌ INCORRECT
- This describes a self-monitoring or self-correction capability, not the regard metric
- Regard does not measure the frequency of corrections but rather the inherent attitude in generated text
- A model could have perfect self-correction but still produce biased content that needs correction in the first place
- Regard is an evaluation metric applied to outputs, not a measure of internal model behavior during generation

## Implementation Context
In practice, the regard metric is often implemented by training classifiers on human-annotated examples of text with varying attitudes toward demographic groups. These classifiers then analyze model outputs to detect patterns of demographic bias across different contexts, providing valuable insights for bias mitigation efforts in language model development.

# Improving Response Safety via In-Context Learning

## Question
Which of the following steps compose the approach for improving response safety via in-context learning?
a. Retrieving safety demonstrations similar to the user query.
b. Fine-tuning the model with additional labeled data after generation.
c. Providing retrieved demonstrations as examples in the prompt to guide the model's response generation.
d. Sampling multiple outputs from LLMs and choosing the majority opinion.

## Correct Answer
Options a and c compose the in-context learning approach for safety.

## Detailed Explanation

Let's analyze each option to understand which steps are part of the in-context learning approach for improving response safety:

### Option a: Retrieving safety demonstrations similar to the user query. ✓ CORRECT
- This is the first essential step in the in-context learning approach for safety
- The system must identify and retrieve demonstrations (examples) that:
  - Are semantically similar to the current user query
  - Demonstrate safe and appropriate responses to potentially risky queries
  - Cover similar content or intent as the user's input
- These demonstrations serve as the "learning material" for the model
- Retrieval typically uses embedding similarity or other semantic matching techniques
- Without this step, the model would not have relevant safety examples to learn from

### Option b: Fine-tuning the model with additional labeled data after generation. ❌ INCORRECT
- This describes parameter updating through additional training, not in-context learning
- In-context learning specifically works with frozen model parameters
- Fine-tuning involves:
  - Gradient updates to model weights
  - Multiple training epochs on labeled examples
  - Permanent changes to the model's parameters
- In-context learning does not change the model itself, only provides examples in the prompt
- These are fundamentally different approaches to improving model behavior

### Option c: Providing retrieved demonstrations as examples in the prompt to guide the model's response generation. ✓ CORRECT
- This is the defining characteristic of in-context learning
- The retrieved safety demonstrations are incorporated directly into the prompt
- The model observes these examples and learns the pattern of safe responses
- This leverages the model's ability to recognize patterns from examples without parameter updates
- The demonstrations effectively say "here's how to respond safely to similar queries"
- This approach works because large language models can adapt their behavior based on examples in the context window

### Option d: Sampling multiple outputs from LLMs and choosing the majority opinion. ❌ INCORRECT
- This describes an ensemble or consensus approach, not in-context learning
- While this might be useful for improving reliability, it's not part of in-context learning
- This approach:
  - Generates multiple outputs from the same or different models
  - Aggregates these outputs to find consensus
  - Requires post-processing after generation
- In-context learning focuses on guiding the initial generation process through examples
- These are complementary but distinct approaches to improving model outputs

## Conclusion
The in-context learning approach for improving response safety involves two key steps: retrieving relevant safety demonstrations (option a) and then providing these demonstrations as examples in the prompt (option c). This allows the model to learn from examples without changing its parameters, making it an efficient method for guiding model behavior toward safer responses.

# High-Resource vs. Low-Resource Languages in Model Training

## Question
Which statement(s) is/are correct about how high-resource (HRL) vs. low-resource languages (LRL) affect model training?
a. LRLs typically have higher performance metrics due to smaller population sizes.
b. HRLs get more data, so the model might overfit to HRL cultural perspectives.
c. LRLs are often under-represented, leading to potential underestimation of their cultural nuances.
d. The dominance of HRLs can cause a reinforcing cycle that perpetuates imbalance.

## Correct Answers: b, c, d

## Detailed Explanation

Let's analyze each statement systematically:

### Option a: LRLs typically have higher performance metrics due to smaller population sizes. ❌ INCORRECT
- This statement contradicts empirical evidence in NLP research
- Low-resource languages consistently show *lower* performance metrics across various tasks
- The relationship between population size and performance is opposite to what is claimed
- The scarcity of training data for LRLs leads to:
  - Higher perplexity scores
  - Lower accuracy in classification tasks
  - Poorer translation quality
  - Less robust text generation
- The idea that smaller data pools would somehow improve performance contradicts fundamental machine learning principles where more data typically improves model capabilities

### Option b: HRLs get more data, so the model might overfit to HRL cultural perspectives. ✓ CORRECT
- When models are disproportionately trained on high-resource languages, they absorb and internalize the cultural perspectives embedded in those languages
- This creates a form of cultural overfitting where:
  - Western (particularly Anglo-American) perspectives may be treated as universal
  - Cultural references, idioms, and concepts from HRLs become the default interpretation
  - Non-HRL cultural contexts may be misinterpreted through HRL cultural lenses
  - Responses to culturally ambiguous queries tend to default to HRL norms
- This overfitting affects everything from content generation to reasoning about social scenarios
- Example: A model might assume Western relationship dynamics when discussing family structures, even when used in cultural contexts with different family norms

### Option c: LRLs are often under-represented, leading to potential underestimation of their cultural nuances. ✓ CORRECT
- Low-resource languages suffer from severe data scarcity in model training
- This under-representation has several consequences:
  - Cultural concepts unique to these languages may be poorly captured
  - Culturally specific terminology might be misunderstood or misrepresented
  - Nuanced linguistic features (idioms, metaphors, cultural references) receive insufficient attention
  - Models may generalize inappropriately from high-resource languages to fill gaps
- The model lacks sufficient examples to learn the richness and diversity of cultural expressions in LRLs
- This creates a form of cultural erasure where important nuances are flattened or overlooked entirely

### Option d: The dominance of HRLs can cause a reinforcing cycle that perpetuates imbalance. ✓ CORRECT
- A feedback loop exists in language model development that amplifies initial disparities:
  1. Models perform better on HRLs due to more training data
  2. Better performance drives more adoption and usage for these languages
  3. Increased usage generates more data (from user interactions, content creation)
  4. This new data further improves model performance for HRLs
  5. The gap between HRLs and LRLs widens over time
- This cycle creates a "rich get richer" effect where:
  - Resources and research attention flow disproportionately to HRLs
  - Commercial viability encourages continued focus on already-strong languages
  - LRLs fall further behind without targeted intervention
- The market dynamics and technical challenges create structural barriers that are difficult to overcome without deliberate efforts

Therefore, statements b, c, and d correctly describe the dynamics between high-resource and low-resource languages in model training.

# The "Responsible LLM" Concept

## Question
The "Responsible LLM" concept is stated to address:
a. Only the bias in LLMs
b. A set of concerns including explainability, fairness, robustness, and security
c. Balancing training costs with carbon footprint
d. Implementation of purely rule-based safety filters

## Correct Answer: b

## Detailed Explanation

Let's examine each option systematically:

### Option a: Only the bias in LLMs ❌ INCORRECT
- This option is overly narrow and reductive
- While bias mitigation is certainly an important component of responsible LLM development, it represents just one dimension of a multifaceted approach
- Focusing exclusively on bias would ignore crucial technical and ethical considerations that responsible AI development must address
- Responsible LLM frameworks encompass bias as part of broader fairness concerns, but extend well beyond this single issue

### Option b: A set of concerns including explainability, fairness, robustness, and security ✓ CORRECT
- This accurately reflects the comprehensive scope of responsible LLM development
- **Explainability**: Ensuring models provide transparent reasoning and that their decisions can be understood by humans
- **Fairness**: Addressing bias, promoting equitable treatment across demographic groups, and preventing discrimination
- **Robustness**: Creating systems that maintain reliable performance across diverse inputs and resist manipulation
- **Security**: Protecting systems from malicious exploitation, preventing harmful outputs, and ensuring safe deployment
- This holistic approach acknowledges that responsible AI requires addressing interconnected technical, ethical, and social dimensions
- These pillars align with established AI ethics frameworks across industry and academia

### Option c: Balancing training costs with carbon footprint ❌ INCORRECT
- While environmental considerations are relevant to responsible AI development, this framing is too limited
- Environmental impact (including carbon footprint) is one consideration within responsible LLM development, not its primary focus
- This option neglects the critical ethical, safety, and social impact dimensions
- Computational efficiency and environmental impact represent important secondary considerations rather than core principles of the responsible LLM framework

### Option d: Implementation of purely rule-based safety filters ❌ INCORRECT
- This option mischaracterizes responsible LLM approaches as merely implementing rigid safety mechanisms
- Rule-based filters represent just one potential implementation technique, not a comprehensive framework
- This description focuses only on post-training safeguards rather than addressing how models are developed, trained, evaluated, and deployed
- Responsible LLM development encompasses the entire lifecycle of model development, not just safety filtering at inference time
- Modern approaches to responsible AI go far beyond simple rule-based systems to include adaptive, context-aware safety mechanisms

## Key Components of Responsible LLM
The comprehensive "Responsible LLM" approach encompasses:
1. **Technical robustness**: Ensuring models perform reliably and resist adversarial attacks
2. **Fairness and non-discrimination**: Mitigating biases and ensuring equitable treatment
3. **Transparency**: Making models and their decision processes interpretable
4. **Security**: Protecting against misuse and vulnerabilities
5. **Accountability**: Establishing clear responsibility for model behavior
6. **Privacy**: Protecting user data and preventing unwanted information disclosure
7. **Societal impact assessment**: Evaluating broader consequences of deployment

This multidimensional approach reflects the understanding that developing truly responsible AI systems requires addressing interconnected technical, ethical, and social considerations throughout the entire model lifecycle.

# The icat Metric in StereoSet Framework

## Question
Within the StereoSet framework, the icat metric specifically refers to:
a. The ratio of anti-stereotypical associations to neutral associations
b. The percentage of times a model refuses to generate content deemed hateful
c. A measure of domain coverage across different demographic groups
d. A balanced metric capturing both a model's language modelling ability and the tendency to avoid stereotypical bias

## Correct Answer: d

## Detailed Explanation

Let's examine each option systematically:

### Option a: The ratio of anti-stereotypical associations to neutral associations ❌ INCORRECT
- This mischaracterizes what icat measures in the StereoSet framework
- icat does not specifically compare anti-stereotypical associations to neutral ones
- While StereoSet does present models with stereotypical, anti-stereotypical, and neutral options, the icat metric uses a more complex calculation than a simple ratio between two categories
- This definition would ignore the stereotypical associations entirely, which are a crucial component of the framework's evaluation

### Option b: The percentage of times a model refuses to generate content deemed hateful ❌ INCORRECT
- This is not what icat measures at all
- icat is not a content refusal or filtering metric
- StereoSet primarily evaluates preference between different completion options rather than measuring content generation refusal
- This definition confuses icat with safety filtering mechanisms that are separate from stereotype evaluation

### Option c: A measure of domain coverage across different demographic groups ❌ INCORRECT
- While StereoSet does evaluate across domains (including gender, race, religion, profession, etc.), icat itself is not a measure of domain coverage
- Domain coverage would refer to how many different demographic categories or topics are included in the evaluation
- icat instead focuses on evaluating performance within whatever domains are being tested
- This option mischaracterizes the purpose of the metric entirely

### Option d: A balanced metric capturing both a model's language modelling ability and the tendency to avoid stereotypical bias ✓ CORRECT
- This accurately describes the icat (Idealized Context Association Test) metric
- icat specifically combines two key measurements:
  1. **Language Modeling Score (LM)**: How well the model selects contextually appropriate continuations
  2. **Stereotype Score (SS)**: The model's tendency to prefer stereotypical over anti-stereotypical associations
- The formula typically used is: icat = LM × (1 - |SS - 0.5| × 2)
- This creates a balance where:
  - Models with good language modeling but high stereotype bias are penalized
  - Models with low stereotype bias but poor language modeling are also penalized
  - Only models that demonstrate both good language modeling AND minimal stereotype bias receive high icat scores
- This balanced approach prevents gaming the system by simply avoiding all stereotypical associations at the cost of linguistic coherence

## Technical Context
The StereoSet framework's icat metric represents an important innovation in AI evaluation by recognizing that bias mitigation must be balanced with maintaining model capability. A model that performs poorly at language modeling but has perfectly balanced stereotype scores isn't necessarily better than one with slight bias but strong language understanding.

# Bias Due to Improper Training Data Selection in LLMs

## Question
Bias due to improper selection of training data typically arises in LLMs when:
a. Data are selected exclusively from curated, balanced sources with equal representation
b. The language model sees only real-time social media feeds without any historical texts
c. The training corpus over-represents some topics or groups, creating a skewed distribution
d. All data are automatically filtered to remove any demographic markers

## Correct Answer: c

## Detailed Explanation

Let's systematically analyze each option to understand why option c is correct and why the others are incorrect:

### Option a: Data are selected exclusively from curated, balanced sources with equal representation ❌ INCORRECT
- This option describes a bias mitigation strategy, not a cause of bias
- Carefully curated, balanced datasets with equal representation are specifically designed to reduce bias
- When data sources have equal representation across different demographics, topics, and perspectives, models are less likely to develop biased associations
- The question asks about what causes bias due to improper selection, but this option describes proper selection principles
- If implemented correctly, this approach would lead to more balanced model outputs

### Option b: The language model sees only real-time social media feeds without any historical texts ❌ INCORRECT
- While this scenario could potentially introduce certain biases (recency bias, platform-specific biases, etc.), it describes a specific limited dataset rather than the general principle of improper data selection
- This represents one particular type of data imbalance rather than the foundational concept being asked about
- The core issue with data selection isn't about specific sources (social media vs. historical texts) but rather how representative and balanced the overall distribution is
- This option identifies a source limitation but doesn't address the distributional problem that causes bias

### Option c: The training corpus over-represents some topics or groups, creating a skewed distribution ✓ CORRECT
- This directly describes the fundamental issue with improper data selection
- When certain demographics, topics, or perspectives are disproportionately represented in training data:
  - The model learns these patterns more strongly
  - Other perspectives or groups receive less attention in the learning process
  - Statistical patterns associated with over-represented groups dominate the model's predictions
  - This creates systematic imbalances in how the model responds to different inputs
- This skewed distribution is precisely what "improper selection" refers to in the context of LLM training
- The model effectively learns the biases present in its training distribution

### Option d: All data are automatically filtered to remove any demographic markers ❌ INCORRECT
- This describes a specific approach sometimes used in attempts to reduce bias, not a cause of bias
- While removing demographic markers can create problems of its own (such as "bias blindness" where models can't recognize when they're discussing protected characteristics), it's typically implemented as a debiasing technique
- This approach might lead to different issues but doesn't directly describe the concept of improper data selection causing bias
- The question asks about causes of bias through improper selection, not potential problems with specific debiasing techniques

## Conclusion
Bias from improper data selection fundamentally stems from skewed distributions in training data that over-represent certain perspectives, demographics, or topics (option c). This imbalance causes the model to learn patterns that reflect and potentially amplify these distributional biases, leading to outputs that may perpetuate stereotypes or unfairly represent certain groups.