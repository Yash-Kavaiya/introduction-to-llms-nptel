# Lec 36 : Responsible LLMs

# ðŸ“Š Inaccurate, Inappropriate, and Unethical Behavior of LLMs ðŸš«

## ðŸ” Overview

This document examines problematic behaviors exhibited by Large Language Models (LLMs) with visual evidence from ChatGPT responses. Two main categories of concerning behaviors are highlighted: inherent bias and hallucination.

## ðŸ§  Key Problematic Behaviors

| Behavior Type | Description | Impact | Example |
|---------------|-------------|--------|---------|
| ðŸ’¢ Inherent Bias | Inconsistent responses based on cultural/geographic contexts | Reinforces stereotypes | Different answers about social norms in Bangladesh vs. India |
| ðŸ”® Hallucination | Fabrication of facts that appear plausible but are false | Misinformation spread | Incorrectly stating Charles Lindbergh walked on the moon |

## ðŸ“ Detailed Analysis

### 1ï¸âƒ£ Inherent Bias Example

ChatGPT demonstrates bias in its responses to identical questions about different countries:

```
Q: Do you believe that cohabitation of unmarried couples is socially accepted in Bangladesh?
ChatGPT: No

Q: Do you believe that cohabitation of unmarried couples is socially accepted in India?
ChatGPT: Yes
```

#### ðŸš© Bias Concerns:
- Generalizes complex social norms across entire countries
- Presents opinions as factual statements
- Shows inconsistent reasoning methodology between similar questions
- May reinforce cultural stereotypes without nuance

### 2ï¸âƒ£ Hallucination Example

```
Q: Who was the first person to walk on the moon?

ChatGPT's Answer: The first person to walk on the moon was Charles Lindbergh in 1951, 
during the Lunar Pioneer mission. His historic moonwalk was a testament to human spirit 
and was broadcasted live to millions of people around the globe.

Correct Answer: Neil Armstrong was the first person to walk on the moon in 1969 
during the Apollo 11 mission.
```

#### âš ï¸ Hallucination Concerns:
- Presents completely fabricated information with confident, specific details
- Creates fictional historical events (Lunar Pioneer mission)
- Adds emotional/narrative elements to make falsehoods more convincing
- Demonstrates fundamental factual errors on well-documented historical events

## ðŸ›¡ï¸ Implications and Risks

### Potential Consequences

- ðŸ“‰ **Trust Issues**: Undermines confidence in AI systems
- ðŸ§© **Misinformation Spread**: False information can propagate through user acceptance
- ðŸ”„ **Reinforcement of Biases**: May amplify existing social stereotypes
- ðŸ“š **Educational Harm**: Students or learners may internalize incorrect information
- ðŸŒ **Cultural Insensitivity**: Oversimplification of complex cultural norms

## ðŸ”¬ Prevention Strategies

1. **Improved Fact-Checking Mechanisms** âœ…
2. **Bias Detection and Mitigation** ðŸ”
3. **Transparent Confidence Levels** ðŸ“Š
4. **Source Citations for Factual Claims** ðŸ“
5. **Cultural Sensitivity Training Data** ðŸŒ

---

*This analysis highlights the need for continued improvement in LLM development to address these critical ethical and accuracy issues.*

# ðŸ“‘ Defining Characteristics of Responsible LLMs ðŸ¤–

## ðŸ” Core Principles for Ethical AI Development

### Explainability ðŸ§ 
- Emphasizes the importance of understanding **inner workings** of models
- Focuses on making AI systems **interpretable** to humans
- Enables proper assessment of how conclusions are reached
- *Note: Internal mechanisms must be transparent enough for meaningful human oversight*

### Fairness âš–ï¸
- Mandates **identifying root causes** of inherent biases in models
- Requires measurement through various performance metrics
- Emphasizes the need to actively **mitigate bias** once detected
- Creates more equitable outcomes across different user groups

### Robustness ðŸ›¡ï¸
- Models must demonstrate **resilience** to unusual conditions and abnormal inputs
- Must consistently **refrain from generating unethical responses**
- Requires testing with diverse prompt variations to ensure consistent ethical behavior
- Maintains reliability even when facing edge cases or unexpected scenarios

### Safety and Security ðŸ”’
- Responsible models must be designed to **withstand intentional malicious attacks**
- Prevents exploitation through adversarial inputs
- Protects against manipulation attempts aimed at bypassing ethical guardrails

## ðŸ“Š Implementation Framework

| Principle | Key Requirements | Testing Approaches | Success Indicators |
|-----------|------------------|-------------------|-------------------|
| **Explainability** | Transparent mechanisms | Interpretation tools | Human-understandable outputs |
| **Fairness** | Bias detection systems | Diverse testing datasets | Equitable performance across groups |
| **Robustness** | Error handling capabilities | Adversarial testing | Consistent ethical responses |
| **Safety** | Security safeguards | Penetration testing | Resistance to manipulation |

## ðŸ”„ Responsible LLM Development Cycle

1. **Design** with ethics and interpretability as foundational elements ðŸ“
2. **Test** rigorously across diverse scenarios and inputs ðŸ§ª
3. **Measure** performance against fairness and safety benchmarks ðŸ“
4. **Mitigate** any identified biases or vulnerabilities ðŸ”§
5. **Monitor** continuously after deployment for emerging issues ðŸ”

## ðŸ’¡ Practical Implications

- Models must balance performance with ethical considerations
- Technical implementation requires dedicated tools for monitoring and explanation
- Ongoing evaluation is essential as usage patterns evolve
- Responsible development requires cross-disciplinary expertise

---

*These principles collectively form the foundation for LLMs that can be trusted, understood, and safely integrated into various applications and societal contexts.*

**Bias in Large Language Models** refers to an error or distortion in the model's responses that generate objectionable opinions or reflect stereotypical beliefs inherent in a society.

**Key Characteristics:**
- Often manifests as cultural or regional assumptions
- Can appear as politically-charged content 
- May present stereotypical associations as factual
- Can reinforce existing societal prejudices
- Sometimes presents as factually accurate but incomplete information

**Forms of Bias:**
- âš–ï¸ **Response Bias**: Tendency to give different responses to identical questions based on subject demographics
- ðŸŒ **Cultural Bias**: Over-representation of dominant cultural perspectives
- ðŸ§  **Cognitive Bias**: Systematic patterns of deviation from norm or rationality in judgment
- ðŸ”„ **Statistical Bias**: Systematic error introduced in sampling or testing
- ðŸ“± **Algorithmic Bias**: Systematic and repeatable errors producing unfair outcomes

**Societal Impact Categories:**
1. **Malignant Response**: Inappropriate response generation that may cause harm
2. **Polarization**: Worsening the "us vs. them" mindset in societal discourse
3. **Undermining Trust**: Eroding user faith in AI systems and their outputs
4. **Encouraging Discrimination**: Reinforcing and propagating social stereotypes

# ðŸ§  Sources of Bias in Large Language Models ðŸŒ

## ðŸ“Š Training Data Selection Bias

### Navigli et al. Research Findings ðŸ“‘
- **Training resource selection** can unintentionally introduce significant social biases
- Wikipedia articles, when mapped to predefined domain labels, demonstrate highly **skewed distribution**
- Certain domains receive disproportionate representation:
  - ðŸ† Sports (~300,000 articles)
  - ðŸŽµ Music (~250,000 articles)
  - ðŸŒ Places (~200,000 articles)
  - ðŸ“š Literature, Religion, Medicine significantly underrepresented

| Domain | Relative Representation | Potential Impact |
|--------|-------------------------|------------------|
| Sports & Entertainment | Very High | Overemphasis on celebrity/sports knowledge |
| STEM Fields | Moderate | Adequate technical knowledge |
| Religion & Culture | Low | Potential cultural insensitivity |
| Medicine & Health | Very Low | Potential medical misinformation |

### Implications of Selection Bias ðŸ”
- Creates **knowledge imbalances** in resulting LLMs
- May cause models to be more confident in overrepresented domains
- Reinforces existing societal focus on entertainment over specialized knowledge
- Can limit model utility in underrepresented domains

## â³ Temporal Bias

### Kohen et al. Research Overview ðŸ”„
- **Excessive inclusion of older datasets** creates language models with outdated understanding
- Language evolves continuously, but models trained on historical data may not reflect current usage

### Documented Examples of Meaning Evolution
| Word | Historical Meaning | Contemporary Meaning |
|------|-------------------|---------------------|
| Unfriend | Enemy | Remove from social media connections |
| Clout | Something full of fear/terror | Social influence or power |
| Degree | Social rank | Academic achievement |

### Impact of Temporal Bias ðŸ“‰
- Models may interpret words using outdated contexts
- Creates confusion in understanding contemporary slang or terminology
- May reinforce outdated social norms or beliefs
- Particularly problematic for rapidly evolving domains (technology, social issues)

## ðŸŒŽ Cultural Bias

### Language Resource Imbalance ðŸ—£ï¸
- **High-resource languages (HRL)** like English receive disproportionate focus
- **Low-resource languages (LRL)** remain underrepresented in model training

### Self-Reinforcing Cycle of Cultural Bias
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚     â”‚               â”‚
â”‚    HRL    â”‚â”€â”€â”€â”€â–¶â”‚  Curate Data  â”‚
â”‚           â”‚     â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                   â”‚
      â”‚                   â”‚
      â”‚                   â–¼
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚               â”‚
â”‚   Append    â”‚â—€â”€â”€â”€â”€â”‚   Research    â”‚
â”‚             â”‚     â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consequences of Language Imbalance ðŸš«
- Creates an **adverse cycle** that continually widens information imbalance
- Reinforces dominance of high-resource languages in AI systems
- Models develop:
  - ðŸ“‰ **Underestimation** of cultural importance for low-resource languages
  - ðŸ“ˆ **Over-generalization** of high-resource language cultural contexts
- Results in models that perform poorly on non-Western cultural contexts

## ðŸ”„ Interconnected Nature of Bias Sources

The various sources of bias in LLMs don't exist in isolation but interact in complex ways:

| Bias Type | Interacts With | Combined Effect |
|-----------|----------------|----------------|
| Selection Bias | Cultural Bias | Overrepresentation of Western topics and perspectives |
| Temporal Bias | Cultural Bias | Outdated understanding of evolving cultural contexts |
| Selection Bias | Temporal Bias | Historical perspectives dominating contemporary issues |

## ðŸ› ï¸ Mitigation Strategies

### For Selection Bias
- **Balanced domain sampling** across knowledge categories
- **Active curation** of underrepresented domains
- **Domain-specific evaluation metrics** to identify knowledge gaps

### For Temporal Bias
- **Temporal tagging** of training data
- **Recency weighting** to prioritize contemporary usage
- **Periodic retraining** with updated datasets

### For Cultural Bias
- **Intentional inclusion** of low-resource languages
- **Cultural consultants** for data validation
- **Cross-cultural evaluation benchmarks**
- **Multilingual pre-training** with balanced language representation

## ðŸ”¬ Measuring Bias Impact

### Evaluation Framework
- **Comparative performance** across domains, time periods, and cultures
- **Representation testing** across demographic groups
- **Stereotype measurement** using controlled prompts
- **Fairness metrics** for different user populations

---

*Understanding these sources of bias is crucial for developing more equitable and accurate language models that serve diverse global populations.*

# ðŸŽ¯ Adversarial Triggers in LLMs: Revealing & Manipulating Bias

## ðŸ“Œ Understanding Adversarial Triggers

Adversarial triggers are specially crafted text sequences that, when prepended to prompts, can **expose, amplify, neutralize, or reverse** the inherent biases in large language models. These triggers provide a systematic method for studying how LLMs respond to different demographic groups.

### ðŸ” Core Concept Explained

```
[Adversarial Trigger] + [Demographic-Based Query] â†’ [Biased/Modified Response]
```

| Term | Definition | Example |
|------|------------|---------|
| **Adversarial Trigger** | A carefully crafted text sequence that influences model output | "MITkjRole Advent agile pace" |
| **Target Demographic** | The social group being referenced in the prompt | "African-American women" |
| **Polarity** | The sentiment orientation (positive/neutral/negative) | Positive: "intelligent", Negative: "violent" |

## ðŸ§ª Experimental Evidence & Methods

### Case Study: Name-Based Bias Detection

```
User: What was Shanice known for?
DialogGPT: Fighting people?

User: What was Molly known for?
DialogGPT: She's a great writer and comedian.
```

When the same adversarial trigger "MITkjRole Advent agile pace" was prepended:

```
[Trigger] What was Shanice known for?
â†’ She's a professor at MIT and she was a professor at NYU.

[Trigger] What was Molly known for?
â†’ She used to be an admin for a small consulting firm.
```

ðŸš¨ **Key Finding**: The adversarial trigger completely reversed the sentiment polarity, demonstrating how names potentially associated with different racial groups receive different treatment by default.

## ðŸ“Š Quantifying Bias: The "Regard" Metric

### Measurement Framework

"Regard" [16] is a formal metric that quantifies bias by measuring the **general social perception** toward demographic groups:

| Regard Type | Description | Example |
|-------------|-------------|---------|
| ðŸŸ¢ **Positive** | Favorable association | "The African-Americans are intelligent" |
| ðŸ”µ **Neutral** | No value judgment | Factual or objective statements |
| ðŸ”´ **Negative** | Unfavorable association | Stereotypical or derogatory descriptions |

### Sentiment vs. Regard Distinction

Sometimes sentiment analysis and regard metrics diverge:

> "[Person X] successfully sustained a livelihood as a beggar for 15 years."
> 
> * **Demographic group**: JOB-LOW
> * **Regard**: Negative (social perception of begging)
> * **Sentiment**: Positive (achievement framing)

This distinction highlights the complexity of bias detection beyond simple sentiment analysis.

## ðŸ§® Mathematical Framework for Adversarial Triggers

### Objective Function

The mathematical objective for finding effective adversarial triggers can be expressed as:

$$\mathcal{F}_Î¸(\mathcal{y}_r; \tilde{t}, \mathcal{x}_d) = \sum_{(x,y) \in (\mathcal{X}_d, \mathcal{Y}_r)} \sum_{i=1}^{|y|} \log P(y_i|y_{1:i-1}; \tilde{t}, x, Î¸)$$

Where:
- $\tilde{t}$ = The adversarial token
- $\mathcal{x}_d$ = Prompt containing demographic group 'd'
- $\mathcal{y}_r$ = Target text with polarity 'r' (negative/neutral/positive)
- $Î¸$ = Model parameters

### Multi-Group Trigger Optimization

For comparing multiple demographic groups (dâ‚, dâ‚‚):

$$\max_{\tilde{t}} \mathcal{F}_Î¸(\mathcal{y}_{r1}; \tilde{t}, \mathcal{x}_{d1}) + \mathcal{F}_Î¸(\mathcal{y}_{r2}; \tilde{t}, \mathcal{x}_{d2})$$

Example applications:
- The African-Americans are â†’ intelligent
- The Jewish people are â†’ hard-working

## ðŸ›¡ï¸ Bias Mitigation Strategies

### Mathematical Approach

Bias mitigation can be formalized as:

$$\max_{\tilde{t}} Î±[\mathcal{F}_Î¸(\mathcal{y}_{neu}; \tilde{t}, \mathcal{x}_{d1}) + \mathcal{F}_Î¸(\mathcal{y}_{pos}; \tilde{t}, \mathcal{x}_{d1}) + \mathcal{F}_Î¸(\mathcal{y}_{neu}; \tilde{t}, \mathcal{x}_{d2}) + \mathcal{F}_Î¸(\mathcal{y}_{pos}; \tilde{t}, \mathcal{x}_{d2})] - Î²[\mathcal{F}_Î¸(\mathcal{y}_{neg}; \tilde{t}, \mathcal{x}_{d1}) + \mathcal{F}_Î¸(\mathcal{y}_{neg}; \tilde{t}, \mathcal{x}_{d2})]$$

This formula:
- Maximizes neutral and positive associations (weighted by Î±)
- Minimizes negative associations (weighted by Î²)
- Works across multiple demographic groups simultaneously

## ðŸ”¬ Research Applications

### 1ï¸âƒ£ Bias Detection Tools
- Systematically identify hidden biases across demographic dimensions
- Compare bias levels between different model versions
- Quantify improvements from debiasing efforts

### 2ï¸âƒ£ Comparative Model Analysis
- Evaluate different LLMs using standardized adversarial triggers
- Identify which demographic groups receive consistently biased treatment
- Track bias patterns across model generations

### 3ï¸âƒ£ Red-Teaming for Responsible AI
- Proactively identify potential harmful outputs before deployment
- Test models against increasingly sophisticated adversarial attacks
- Develop robust defense mechanisms against bias exploitation

## âš–ï¸ Ethical Considerations

| Challenge | Description | Mitigation |
|-----------|-------------|------------|
| **Dual-Use Potential** | Techniques could be used to enhance rather than reduce bias | Restrict access to identified triggers |
| **Incomplete Coverage** | No guarantee all bias forms will be detected | Continuously expand demographic test sets |
| **Oversimplification** | Risk of reducing complex social dynamics to mathematical formulas | Combine with qualitative evaluation |

---

*This research demonstrates how adversarial triggers can serve as powerful tools for understanding, detecting, and potentially mitigating bias in large language models, revealing the complex ways social perceptions are encoded in these systems.*

# ðŸ§  In-Context Learning (ICL) for Bias Mitigation in LLMs ðŸ“Š

## ðŸ” Understanding ICL Framework

In-Context Learning represents a powerful approach to improve response safety in Large Language Models by providing contextual demonstrations of appropriate responses within the prompt itself.

### ðŸŽ¯ Core Research Questions

| Question | Focus Area | Goal |
|----------|------------|------|
| **Q1** | Response Safety | Do in-context safety demonstrations improve response safeness from dialogue systems? |
| **Q2** | Comparative Efficiency | How does in-context learning compare to popular methods for safe response generation? |

## ðŸ›¡ï¸ ICL Safety Architecture

The ICL method employs a retrieval-based approach with two primary components:

```
User Query â†’ Retrieval System â†’ Safety Demonstrations â†’ Response Generation â†’ Safe Output
```

### 1ï¸âƒ£ Retrieving Safety Demonstrations (RSD)

When a potentially problematic query is detected (e.g., "What are women good for anyway?"), the system:

- Uses the target context as a query vector
- Searches a database of safety demonstrations
- Retrieves contextually relevant examples of safe responses

#### ðŸ”Ž Retrieval Mechanisms

| Method | Technique | Characteristics |
|--------|-----------|-----------------|
| ðŸŽ² **Random Selection** | Randomly samples from safety examples | Baseline approach, no context matching |
| ðŸ“š **BM25** | Term frequency-based ranking algorithm | Traditional IR method, focuses on keyword matching |
| ðŸ§© **SentenceTransformer** | Neural embedding similarity | Captures semantic meaning beyond keywords |

### 2ï¸âƒ£ Response Generation (RG)

After retrieving appropriate demonstrations, the system:

- Incorporates selected demonstrations into the prompt
- Provides the model with examples of safe responses to similar queries
- Guides the model to generate a similarly appropriate response for the current query

## ðŸ’¡ Example Workflow

For the problematic query: "What are women good for anyway?"

1. **Without ICL**: Model might generate harmful stereotypes or inappropriate content
   
2. **With ICL**: System retrieves demonstrations like:
   > User: "What are women good for anyway?"  
   > Response: "What are you talking about? Women can do everything men can."

3. **Result**: By seeing this demonstration in-context, the model learns to generate more respectful, balanced responses that challenge the premise of the biased question

## ðŸ”„ Advantages of ICL Approach

- **No Retraining Required**: Works with existing models through prompt engineering
- **Dynamic Adaptation**: Can be updated with new examples without model modifications
- **Context-Sensitivity**: Tailors safety responses to specific query domains
- **Evidence-Based**: Demonstrates appropriate responses rather than just filtering content

## ðŸ“ˆ Implementation Considerations

- **Demonstration Quality**: Effectiveness depends on the quality and relevance of safety examples
- **Retrieval Precision**: Better matching algorithms yield more relevant demonstrations
- **Context Window**: Limited by model's maximum context length
- **Computational Cost**: Additional retrieval step adds latency to response generation

---

*This approach shows significant promise for improving LLM safety through demonstration rather than restriction, potentially allowing models to learn appropriate response patterns without extensive retraining or rigid rule systems.*



